\input{preamble.tex}

\begin{document}
	\title{A tutorial of Solving the defined integral problem with OpenMp and Open MPI}
	\author{André Furlan - UNESP - Universidade Estadual Paulista "Júlio de Mesquita Filho"}
	\date{2023-06-01}
	\maketitle
	
	\begin{abstract}
		The Trapezoid rule algorithm used in this tutorial is \textbf{not} the most fast method to calculate an interval defined integral but is a good example of how to make programs runs quicker by using OpenMp or Open MPI for parallelism. Hopefully it will help the understanding of how these libraries works and how to use them. All sources can be accessed at \href{https://github.com/ensismoebius/ComputacaoDeAltoDesempenho}{https://github.com/ensismoebius/ComputacaoDeAltoDesempenho}
	\end{abstract}
	
	\begin{IEEEkeywords}
		OpenMp, Open MPI, Paralelism, C++, C, Tutorial
	\end{IEEEkeywords}
	
	\section{Introduction}
	\par In High Performance Computation (HPC) the goal (other than solve a problem) is to use most of the computation power of the machines involved. Most of the time this means use all processors an its cores at maximum, avoiding idle intervals.
	\par By using the OpenMp \cite{openmp08} library it is possible by using \textbf{\#pragma} compiler directives, turn a sequential program into a parallel one. 
	
	\par When using the Open MPI library, \cite{mpi40} it is necessary to use the "MPI\_" prefixed commands. The "MPI\_" prefix is a convention used in Open MPI implementations, but it is not required by the MPI standard.
	
	\par The problem in focus here is the defined interval integral calculation. To solve it the program must calculate the area bellow the plot by summing up the area of several trapezoids \cite{hildebrand1987introduction}. The more trapezoids fitted, the more precise are the values obtained. And it is here that the parallelism enters: By dividing the work of area calculation in lots of threads its possible to diminish the effective time of execution.
	
	\section{The trapezoid rule}
	\par Be $a$ a point that marks the start of a interval in which the calculation must be done and $b$ its end, than an \textbf{approximation} of the integral value is given by equation \ref{eq:firstApproximation}. In figure \ref{fig:trapezoidalruleillustration} the basic idea is illustrated.\newline
	
	\begin{equation}
		\int^a_b{f(x) dx} \approx (b-a).\dfrac{1}{2}(f(a)+f(b))
		\label{eq:firstApproximation}
	\end{equation}

	\begin{figure}[h]
		\centering
		\includegraphics[width=0.7\linewidth]{images/Trapezoidal_rule_illustration}
		\caption[Approximation of an integral]{In pink the approximation(trapezoid area), in blue the actual function curve. $f(a)$ and $f(b)$ are the trapezoid bases and the interval $a, b$ its height. }
		\label{fig:trapezoidalruleillustration}
	\end{figure}
	
	\par The figure \ref{fig:trapezoidalruleillustration} and the equation \ref{eq:firstApproximation} shows a pretty coarse approximation! What if the $[a, b]$ interval get subdivided (partitioned) like in figure \ref{fig:trapezoidalruleillustration2}? Well, then it is possible to get a value closer to the real one!\newline
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.7\linewidth]{images/Trapezoidal_rule_illustration2}
		\caption[Enhanced approximation of an integral]{In pink the \textbf{enhanced} approximation(trapezoid area), in blue the actual function curve. $f(a), f(b), f(c), f(d), f(e)$ are the trapezoid bases and the intervals $[a, d],[d,c], [c,e], [e,b]$ its heights. Note that now there are 4 trapezoids i.e. 4 areas that must be calculated. }
		\label{fig:trapezoidalruleillustration2}
	\end{figure}

	\par To obtain the result all areas must be summed up, therefore the equation \ref{eq:firstApproximation} turns into \ref{eq:secondApproximation}. So, as many partitions are made, the more sums are added.\newline
	
	\begin{equation}
		\begin{aligned}
			\int^a_b{f(x) dx} \approx 
			&(d-a).\dfrac{1}{2}(f(d)+f(a))+\\
			&(c-d).\dfrac{1}{2}(f(c)+f(d))+\\
			&(e-c).\dfrac{1}{2}(f(e)+f(c))+\\
			&(b-e).\dfrac{1}{2}(f(b)+f(e))
		\end{aligned}
		\label{eq:secondApproximation}
	\end{equation}

	\par Let $x = [a,b]$ and $n=partitions+1=points$ of $[a,b]$ than it is true to say that $x_0 = a < x_1 < ... < x_{n-1} < x_n = b$. Being the interval between two points $\Delta_{x_k} = x_k - x_{k-1}$ than the equation \ref*{eq:secondApproximation} can be rewrote as the equation \ref{eq:genericApproximation}.\newline
	
	\begin{equation}
		\int^a_b{f(x) dx} \approx \sum_{k=1}^{n} \dfrac{f(x_{k-1})+f(x_k)}{2} . \Delta_{x_k}
		\label{eq:genericApproximation}
	\end{equation}

	\par And this is (finally) the idea that will be implemented ahead!
	
	\section{Implementation}
		\par First of all it is necessary to define a function to integrate, the one used here is defined in equation \ref{eq:function} and the integration interval begins in \textbf{0 until 100}. The performance will be tested using \textbf{1, 2, 4 and 8 threads}.  Note that this method supports any function, this one was chosen for example purposes only.\newline
		
		\begin{equation}
			f(x) = \sqrt{100^2 - x^2}
			\label{eq:function}
		\end{equation}	
	
		\subsection{OpenMp implementation}	
			\lstinputlisting[language=C++]{code/integral.cpp}
		
		\subsection{Open MPI implementation}	
			\lstinputlisting[language=C++]{code/integral2.cpp}
			
	\section{Discussion}
		\par This section focus only in the OpenMp and Open MPI directives, its assumed that the reader knows a minimum of C++.\newline
		
		\subsection{OpenMp discussion}
		
		\par The lines 5 to 7 includes OpenMp header files if the build system used detected such library.
		\begin{lstlisting}[language=C++]
#ifdef _OPENMP
	#include <omp.h>
#endif
		\end{lstlisting}
	
		\par From line 9 to 12, the function to be integrated is defined. Note the \textit{inline} statement: Inlinning functions \textbf{may} avoid the intrinsic overhead of calling a function but, since this it is not a command, the compiler may ignore this request.
		\begin{lstlisting}[language=C++]
inline long double f(long double x)
{
	return std::sqrt(std::pow(100,2) - std::pow(x,2));
}
		\end{lstlisting}
		
		\par Line 22 states that the variable \textit{sum} is shared by all threads all the other variables are not.
		\begin{lstlisting}[language=C++]
#pragma default(none) shared(soma)
		\end{lstlisting}
		
		\par At the 24 position OpenMp starts a parallel zone: From now on the instructions are replicated in several  threads according to parallelization rules defined in the \textbf{operational system ambient variables} \cite{openmp08SV}. The zones are defined using curling braces (same as blocks in C++).
		\begin{lstlisting}[language=C++]
#pragma omp parallel
		\end{lstlisting}
	
		\par The most important part is declared here in line 26. This directive is the heart of the parallelization for this algorithm. For each thread \textit{sum} is initialized to 0 then, after all calculations, the obtained values are summed together. For this directive it have to be possible to known beforehand the number of iterations in execution time i.e. the number of iterations cannot change while the loop are being executed. Also the k variable (used in for loop) must \textbf{not} be modified within the loop otherwise it can be changed like in k++ statement. Since, in this specific case, a summation is need OpenMp have a convenient \textit{reduction} directive that enables it. The \textit{reduction} directive can be used with other types of operations like \textit{subtractions}, \textit{maximums} and \textit{minimums}. 
		
		\begin{lstlisting}[language=C++]
#pragma omp for reduction(+ : sum)
		\end{lstlisting}
	
		\par Finally, at the line 31, a barrier to the execution of the program is created in order to ensure that all threads has been finished until this point. Now it is possible to do another operations with the outputted sum.  This is optional in this \textbf{specific case} because there is an implicit barrier in \textit{for} directive. It is important to note that \textbf{there is no code} associated with this directive i.e. it just marks a point where all threads must arrive. \textbf{Warning!} All or none of the threads must encounter the barrier otherwise a \textbf{deadlock} happens.
		
		\begin{lstlisting}[language=C++]
#pragma omp barrier
		\end{lstlisting}
	
		\par Among the aforementioned \textbf{operational system ambient variables} one is the most important for the sake of this program: OMP\_NUM\_THREADS. This variable sets the number of threads (an integer) that are going to be used when running the algorithm. If no value is assigned than the maximum of threads supported are used.
		
		\subsection{Open MPI discussion}
		\par On line 4 there is a \textbf{required} include for the Open MPI library.
		\begin{lstlisting}[language=C++]
#include <mpi.h>
		\end{lstlisting}
	
		\par From line 6 to 9, the function to be integrated is defined. Note the \textit{inline} statement: Inlinning functions \textbf{may} avoid the intrinsic overhead of calling a function but, since this it is not a command, the compiler may ignore this request.
		
		\begin{lstlisting}[language=C++]
inline long double f(long double x)
{
	return std::sqrt(std::pow(100,2) - std::pow(x,2));
}
		\end{lstlisting}
	
	
		\par On the nice 69 line there is a command that initializes the MPI environment, including communication channels and resources necessary for running parallel Open MPI programs. The "MPI\_Init" command should be called exactly once per MPI process and has the same arguments as the C++ main function i.e. the program arguments count and the arguments.
		
		\begin{lstlisting}[language=C++]
MPI_Init(&argc, &argv);
		\end{lstlisting}


		\par At line 73 and 74 there is a slot identificantion retrieval. A slot is a resource to make computation (usually an CPU core) locally or remotely available. This slot information will be used in the next lines.
		
	\begin{lstlisting}[language=C++]
int slotId;
MPI_Comm_rank(MPI_COMM_WORLD, &slotId);
	\end{lstlisting}
	
	\par On line 76 to 78 the intended message is only shown if the process is from slot 0 i.e. the main process.
	\begin{lstlisting}[language=C++]
 if (slotId == 0) {
	std::cout << "Interval \t Partitions \t Time taken \t Result" << std::endl;
}
	\end{lstlisting}

	\par The line 51 begins the time measurement.
	
	\begin{lstlisting}[language=C++]
double start_time = MPI_Wtime();
	\end{lstlisting}

	\par Line 55 and 56 measure time again and calculates the elapsed time respectively.
	
	\begin{lstlisting}[language=C++]
double end_time = MPI_Wtime();
double elapsed_time = end_time - start_time;
	\end{lstlisting}

	\par As already explained from lines 58 to 64 it guarantees that the elapsed times outputs only runs on main slot. 

	\begin{lstlisting}[language=C++]
int rank;
MPI_Comm_rank(MPI_COMM_WORLD, &rank);

if (rank == 0) {
	std::cout << std::fixed << std::setprecision(6) << interval << " \t " << partitions
	<< " \t " << elapsed_time << " \t " << result << std::endl;
}
	\end{lstlisting}

	\par Line 19 creates the slots variables (\textit{slotId} as the current slot/process identification and \textit{slotsAvailable} as the number of available slots). Lines 20 and 21 retrieves these values.
	
	\begin{lstlisting}[language=C++]
int slotId, slotsAvailable;
MPI_Comm_rank(MPI_COMM_WORLD, &slotId);
MPI_Comm_size(MPI_COMM_WORLD, &slotsAvailable);
	\end{lstlisting}

	\par The line 35 aggregates all sums from \textit{localSum} variables across the slots in the \textit{sum} variable.
	
	\begin{lstlisting}[language=C++]
MPI_Reduce(&localSum, &sum, 1, MPI_LONG_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
	\end{lstlisting}

	\par Finally at 84 line all resources allocated are freed.
	\begin{lstlisting}[language=C++]
MPI_Finalize();
	\end{lstlisting}

	\par Note that most of the processing is done in lines 31 to 33, and the problem \textbf{must} be partitioned manually in Open MPI, which is different from OpenMP. 

	\section{Performance tests}
		\par The tests were done using an \textit{AMD Ryzen 5 5000 series} processor running an \textit{Gnu/Linux operational system} with \textit{30GB of RAM}.\newline
		
		\subsection{OpenMp performance tests}
		
		\par The tables bellow shows the results according to the number of threads. 
		
		\begin{table}[h]
			\caption{export OMP\_NUM\_THREADS=1}
			\begin{center}
				\begin{tabular}{|l|l|l|l|}
					\hline
					Interval $\Delta_{x_k}$  &  Partitions  &  Time taken in sec &  Result \\
					\hline
					0.000001  &  100000000  &  0.670965  &  7853.981634 \\
					\hline
					0.000010  &  10000000  &  0.067101  &  7853.981633 \\
					\hline
					0.000100  &  1000000  &  0.006715  &  7853.981617 \\
					\hline
				\end{tabular}
			\end{center}
		\end{table}
		
		\begin{table}[h]
			\caption{export OMP\_NUM\_THREADS=2}
			\begin{center}
				\begin{tabular}{|l|l|l|l|}
					\hline
					Interval $\Delta_{x_k}$  &  Partitions  &  Time taken in sec &  Result \\
					\hline
					0.000001  &  100000000  &  0.341421  &  7853.981634 \\
					\hline
					0.000010  &  10000000  &  0.033588  &  7853.981633 \\
					\hline
					0.000100  &  1000000  &  0.003378  &  7853.981617 \\
					\hline
				\end{tabular}
			\end{center}
		\end{table}
		
		\begin{table}[h]
			\caption{export OMP\_NUM\_THREADS=4}
			\begin{center}
				\begin{tabular}{|l|l|l|l|}
					\hline
					Interval $\Delta_{x_k}$  &  Partitions  &  Time taken in sec &  Result \\
					\hline
					0.000001  &  100000000  &  0.172951  &  7853.981634 \\
					\hline
					0.000010  &  10000000  &  0.016810  &  7853.981633 \\
					\hline
					0.000100  &  1000000  &  0.001684  &  7853.981617 \\
					\hline
				\end{tabular}
			\end{center}
		\end{table}
			
		\begin{table}[h]
			\caption{export OMP\_NUM\_THREADS=8}
			\begin{center}
				\begin{tabular}{|l|l|l|l|}
					\hline
					Interval $\Delta_{x_k}$  &  Partitions  &  Time taken in sec &  Result \\
					\hline
					0.000001  &  100000000  &  0.144975  &  7853.981634 \\
					\hline
					0.000010  &  10000000  &  0.014428  &  7853.981633 \\
					\hline
					0.000100  &  1000000  &  0.001436  &  7853.981617 \\
					\hline
				\end{tabular}
			\end{center}
		\end{table}

	\section{Performance tests}
\par The tests were done using an \textit{AMD Ryzen 5 5000 series} processor running an \textit{Gnu/Linux operational system} with \textit{30GB of RAM}.\newline

\subsection{Open MPI performance tests}

\par In order to change the number of parallel processing units in an Open MPI process, first you need to create a text file that contains the "hosts" and their respective available slots that will be used by Open MPI.  The hosts can be specified using their DNS name or IP address. In this example a file named "hosts.txt" was created.

\begin{lstlisting}
localhost slots=8
\end{lstlisting}

\par But it could be like the one bellow:

\begin{lstlisting}
localhost slots=4
192.168.0.56 slots=10
192.168.0.101 slots=7
\end{lstlisting}

\par When the aforementioned file is created, you can start the processing using the command bellow. Note the use of the "-np" parameter, which represents the number of slots to be used and can be interpreted as the number of threads in Open MPI. If the specified "-np" value is greater than the number of available slots, an error will be raised.

\begin{lstlisting}
mpirun --hostfile hosts.txt -np 1 ./executable
\end{lstlisting}

\par The tables bellow shows the results according to the number of slots. 

\begin{table}[h]
	\caption{mpirun --hostfile hosts.txt -np 1}
	\begin{center}
		\centering
		\begin{tabular}{|l|l|l|l|}
			\hline
			Interval & Partitions & Time taken & Result \\
			\hline
			0.000001 & 100000000 & 0.470636 & 7853.981634 \\
			0.000010 & 10000000 & 0.044703 & 7853.981633 \\
			0.000100 & 1000000 & 0.004453 & 7853.981617 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\begin{table}[h]
	\caption{mpirun --hostfile hosts.txt -np 2}
	\begin{center}
		\begin{tabular}{|l|l|l|l|}
			\hline
			Interval & Partitions & Time taken & Result \\
			\hline
			0.000001 & 100000000 & 0.240130 & 7853.981634 \\
			0.000010 & 10000000 & 0.022352 & 7853.981633 \\
			0.000100 & 1000000 & 0.002229 & 7853.981617 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\begin{table}[h]
	\caption{mpirun --hostfile hosts.txt -np 4}
	\begin{center}
		\begin{tabular}{|l|l|l|l|}
			\hline
			Interval & Partitions & Time taken & Result \\
			\hline
			0.000001 & 100000000 & 0.136689 & 7853.981634 \\
			0.000010 & 10000000 & 0.011185 & 7853.981633 \\
			0.000100 & 1000000 & 0.001120 & 7853.981617 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\begin{table}[h]
	\caption{lmpirun --hostfile hosts.txt -np 8}
	\begin{center}
		\begin{tabular}{|l|l|l|l|}
			\hline
			Interval & Partitions & Time taken & Result \\
			\hline
			0.000001 & 100000000 & 0.116957 & 7853.981634 \\
			0.000010 & 10000000 & 0.009895 & 7853.981633 \\
			0.000100 & 1000000 & 0.000946 & 7853.981617 \\
			\hline
	\end{tabular}
	\end{center}
\end{table}
	
	\section{Conclusion}
		\par The demonstrated example \textbf{does not imply} the superiority of OpenMP over Open MPI or the other way around because, depending on the problem to be solved and the environment involved, one or the other may be more suitable for the task.
		
		\par OpenMP is used when there is a need to utilize most of the local computer's resources for processing. OpenMP programs are simpler to code as well, thanks to the use of \textbf{\#pragma} compiler directives.
		
		\par On the other hand, Open MPI is better suited when the problem needs to be divided into larger pieces and processed across multiple machines. However, it requires the programmer to split the problem, resulting in a more complex and error-prone code.
		
		\par Hope it helps and, for more information, please consult the references.\newline
		
		\par \textit{That is all folks!}
	
	\bibliographystyle{plain}
	\bibliography{references.bib}
\end{document}